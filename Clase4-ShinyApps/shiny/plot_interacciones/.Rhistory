predictBackWard<-dataTest[c("id")]
predictBackWard$median_house_value<-step.modelfinal %>% predict(dataset_test)
#write.csv(predictBackWard,"model_backward3.csv", row.names = FALSE)
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
# Modelo forward
step.model2<-train(median_house_value ~., data = dataset_train,
method = "leapSeq",
tuneGrid = data.frame(nvmax = 1:8),
trControl = train.control)
step.model2$results
step.model2$bestTune
summary(step.model2$finalModel)
summary(step.model2$finalModel)
step.modelfinal2<-lm(median_house_value ~ ocean_proximityINLAND  + longitude + latitude + housing_median_age  +households  +median_income  +bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
summary(step.modelfinal2)
predictForWard<-dataTest[c("id")]
predictForWard$median_house_value<-step.modelfinal2 %>% predict(dataset_test)
predictForWard
#write.csv(predictBackWard,"model_forward.csv", row.names = FALSE
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
step.model3<-train(median_house_value ~., data = dataset_train,
method = "lmStepAIC",
trControl = train.control,
trace = FALSE
)
step.model3$results
step.model3$finalModel
summary(step.model3$finalModel)
![LINK VIDEO](https://youtu.be/Og6Gy_MVTiM)
# Descripción del Problema
El dataset contiene información sobre el censo de vivienda del estado de California de 1990, cada fila o instancia de la misma representa la información recabada por bloque; es decir un conjunto de casas, los campos o variables que la conforman son las siguientes:
library(forecast)
library(tseries)
library(dplyr)
library(xts)
library(lubridate)
set.seed(2021)
#1. Cree un arreglo de 1500 valores con distribución de probabilidad normal con media = 0 y desviación estándar = 5.
arrNorm=rnorm(n=1500, mean=0, sd=5)
# 2. Cree una suma acumulativa de estos valores.
cumNorm = cumsum(arrNorm)
# 3. Muestre una gráfica de la serie temporal.
plot(cumNorm)
# 4. Cree una serie de tiempo con la función ts()
miTS = ts(cumNorm, frequency = 12)
# 5. Verifique la descomposición de la serie tiempo usando una frecuencia de 12.
plot(decompose(miTS))
# 6. Realice un comentario sobre cada uno de los componentes de la serie temporal.
# Tendencia - la gráfica de la tendencia al principio o inclusive antes de llegar a la "mitad" de los datos que se tienen sobre el tiempo, no está tan constante, baja, trata de mantenerse luego tiene una subida hasta que llega al punto que después de bajar logra compensar ello y empieza a ver un incremento hasta cierto punto y trata de estabilizarse hasta final, probablemente, de acuerdo al final de la tendencia, algunas de las aproximaciones a predecir en el forecast muestren una subida o al menos "en subida".
# Seasonality - Si se expande la gráfica de seasonality, se puede ver claro que a cada cierto tiempo hay una estacionalidad, lo cual demuestra que si existe ese patrón en el caso de tal serie temporal.
# Random- Se puede comprobar también que la serie temporal tiene el componente estocástico, que comúnmente se conoce como el componente que no se puede explicar, y se logra apreciar en aquellos puntos de la observación de que no son líneas suaves, sino que claramente serían lo que se logra observar en random.
# 7. Determine se la serie es estacionar o no (stationarity).
adf.test(miTS)
## El valor p, es bajo, pero ni siquiera tan bajo, para considerar que la serie es stationary, inclusive con evaluarla, no se logra apreciar que exista stationary porque no se mantiene constante, por lo que se concluye que no es una serie STATIONARY.
# 8. De no ser estacionar, investigue como es posible resolver este problema usando diferenciación temporal.
diffMiTS = diff(miTS)
plot(decompose(diffMiTS))
adf.test(diffMiTS)
# 9. Muestre y una grafica sobre la serie temporal resultante.
plot(diffMiTS)
df = read.csv("dataOut.csv")
df
unique(df$id_producto)
table(df$id_producto)
## Debido a que son 84 observaciones por cada id producto, en los pasos indicados en el pdf, dicen "38" observaciones, por lo que se tomará las últimas 38 observaciones, sabiendo que cada observación corresponde a un mes en específico.
reproducir_arima = function(df, id){
temp_df = df %>%
filter(id_producto == id) %>%
top_n(38, timestamp) %>%
select(timestamp, produccion)
temp_df$timestamp = ymd(temp_df$timestamp)
temp_ts = ts(temp_df$produccion,start = c(2018, 11), frequency = 12)
temp_ts = tsclean(temp_ts)
diffMiTS = diff(temp_ts)
bestModel = auto.arima(diffMiTS, trace = F)
plot(forecast(bestModel, h=16))
}
for(id in unique(df$id_producto)){
print(id)
reproducir_arima(df, id)
}
head(mtcars)
x <- sample(letters,replace=TRUE,size=10)
print(x)
plot(cars)
num<- 1+3
print(x)
num<- 1+3
La variable num contiene el valor de `r num``
version
reticulate::repl_python()
library(reticulate)
x = 42 * 2
print(x)
quit
use_condaenv("r-reticulate")
library(reticulate)
reticulate::repl_python()
x = 42 * 2
print(x)
quit
reticulate::repl_python()
x = 42 * 2
print(x)
quit
quit
use_condaenv("r-reticulate")
reticulate::repl_python()
import pandas
iris_pandas = pandas.read_csv("iris.txt")
iris_setosa = iris_pandas[iris_pandas['species'] == "setosa"]
iris_setosa.head()
quit
reticulate::repl_python()
import pandas
iris_pandas = pandas.read_csv("iris.txt")
iris_setosa = iris_pandas[iris_pandas['species'] == "setosa"]
iris_setosa.head()
quit
reticulate::repl_python()
py_install("pandas")
import pandas
iris_pandas = pandas.read_csv("iris.txt")
iris_setosa = iris_pandas[iris_pandas['species'] == "setosa"]
iris_setosa.head()
py_install("pandas")
py_install("pandas", "r-reticulate")
py_install("pandas", "r-reticulate")
import pandas
import pandas
py_install("pandas", "r-reticulate")
conda_install("pandas")
conda_install("pandas")
conda_install("pandas")
sad
conda install pandas
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "pandas")
quit
conda_install("r-reticulate", "pandas")
use_condaenv("r-reticulate")
reticulate::repl_python()
import pandas
iris_pandas = pandas.read_csv("iris.txt")
iris_setosa = iris_pandas[iris_pandas['species'] == "setosa"]
iris_setosa.head()
quit
library(ggplot2)
library(dplyr)
py$iris_setosa %>%
ggplot(aes(x=sepal_length,y=sepal_width))+
geom_point()
library(RMySQL)
install.packages("RMySQL")
library(RMySQL)
dbcon <- dbConnect(MySQL(),
host = 'mysqlexamples-instance-1.coclu0fxxexm.us-east-1.rds.amazonaws.com',
user = 'ufm_student',
password = 'yQ2znv',
dbname = 'datasets')
dbcon <- dbConnect(MySQL(),
host = '127.0.0.1',
user = 'root',
password = 'D5ekyngy',
dbname = 'agroserviciodb')
dbDisconnect(dbcon)
install.packages("flexdashboard")
install.packages("flexdashboard")
install.packages("flexdashboard")
library(flexdashboard)
library(flexdashboard)
knitr::kable(iris)
library(ggplot2)
library(dplyr)
iris %>%
ggplot(aes(x=Species, y=Sepal.Width, color = Species))+
geom_boxplot()
iris %>%
ggplot(aes(x=Sepal.Width, y=..density..))+
geom_histogram()+
geom_density()
library(flexdashboard)
library(readr)
earthquake<-read_csv("data/earthquakedata.csv")
library(dplyr)
library(DT)
earthquake %>%
filter(yr==1995) %>%
datatable()
library(leaflet)
install.packages("leaflet")
library(leaflet)
earthquake %>%
filter(yr==1995, mag > 6) %>%
leaflet() %>%
adTitles() %>%
addMarkers(lng = -lon,
lat = -lat,
label = -mag)
earthquake %>%
filter(yr==1995, mag > 6) %>%
leaflet() %>%
addTitles() %>%
addMarkers(lng = ~lon,
lat = ~lat,
label = ~mag)
earthquake %>%
filter(yr==1995, mag > 6) %>%
leaflet() %>%
addTitles() %>%
addMarkers(lng = ~lon,
lat = ~lat,
label = ~mag)
library(leaflet)
earthquake %>%
filter(yr==1995, mag > 6) %>%
leaflet() %>%
addTitles() %>%
addMarkers(lng = ~lon,
lat = ~lat,
label = ~mag)
earthquake %>%
filter(yr==1995, mag > 6) %>%
leaflet() %>%
addTiles() %>%
addMarkers(lng = ~lon,
lat = ~lat,
label = ~mag)
library(readr)
stats<-read_csv("data/academatica_video_stats.csv")
metricas<-
stats %>%
summarise(total_like = sum(likeCount))
valueBox(metricas$total_like)
library(dplyr)
valueBox(metricas$total_like)
metricas<-
stats %>%
summarise(total_like = sum(likeCount),        icon="fa-thumbs-up", color="success")
metricas<-
stats %>%
summarise(total_like = sum(likeCount))
valueBox(metricas$total_like, icon="fa-thumbs-up", color="success")
library(flexdashboard)
library(ggplot2)
library(dplyr)
knitr::kable(iris)
iris %>%
ggplot(aes(x=Species, y=Sepal.Width, fill = Species))+
geom_boxplot()
iris %>%
ggplot(aes(x=Sepal.Width, y=..density.., fill=Species))+
geom_histogram()+
geom_density()+
facet_wrap(.~Species)
plot(cars)
plot(cars)
library(flexdashboard)
library(readr)
library(dplyr)
stats<-read_csv("data/academatica_video_stats.csv")
metricas<-
stats %>%
summarise(total_like = sum(likeCount))
valueBox(formattable::comma(
metricas$total_like, digits = 0), icon="fa-thumbs-up",  color="success")
stats<-read_csv("data/academatica_video_stats.csv")
videos<-read_csv("data/academatica_videos.csv")
metadata<-read_csv("data/academatica_videos_metadata.csv")
metricas<-
stats %>%
summarise(total_like = sum(likeCount))
metricas<-
stats %>%
summarise(total_like = sum(likeCount),
total_views = sum(viewCount),
total_dislikes = sum(dislikeCount),
total_comments = sum(commentCount))
valueBox(
metricas$total_views,
icon = "fa-eye",
color="info"
)
valueBox(
formattable::comma(
metricas$total_views, digits = 0),
icon = "fa-eye",
color="info"
)
valueBox(
formattable::comma(
metricas$total_comments, digits = 0),
icon = "fa-comments",
color="primary"
)
like_rate<-metricas$total_like/(metricas$total_like+metricas$total_dislikes)
like_rate<-round(like_rate*100, 0)
gauge(like_rate, min = 0, max = 100, symbol = "%")
gauge(100 - like_rate, min = 0, max = 100, symbol = "%")
gauge(100 - like_rate,
min = 0,
max = 100, symbol = "%",
gaugeSectors(success=c(80,100),
warning=c(40,79),
danger=c(0,39))
)
like_rate<-metricas$total_like/(metricas$total_like+metricas$total_dislikes)
like_rate<-round(like_rate*100, 0)
gauge(like_rate, min = 0,
max = 100, symbol = "%",
gaugeSectors(success=c(80,100),
warning=c(40,79),
danger=c(0,39))
)
library(ggplot2)
library(lubridate)
videos %>%
mutate(year = year(ymd_hms(contentDetails.videoPublishedAt)),
month = month(ymd_hms(contentDetails.videoPublishedAt)),
year = as.factor(year)
)
videos %>%
mutate(year = year(ymd_hms(contentDetails.videoPublishedAt)),
month = month(ymd_hms(contentDetails.videoPublishedAt)),
year = as.factor(year)
) %>%
group_by(year,month) %>%
summarise(uploaded_videos = n_distinct(contentDetails.videoId))
videos %>%
mutate(year = year(ymd_hms(contentDetails.videoPublishedAt)),
month = month(ymd_hms(contentDetails.videoPublishedAt))
) %>%
group_by(year,month) %>%
summarise(uploaded_videos = n_distinct(contentDetails.videoId))
videos %>%
mutate(year = year(ymd_hms(contentDetails.videoPublishedAt)),
month = month(ymd_hms(contentDetails.videoPublishedAt)),
year = as.factor(year),
month = as.factor(month)
) %>%
group_by(year,month) %>%
summarise(uploaded_videos = n_distinct(contentDetails.videoId)) %>%
ggplot(aes(x=month, y=uploaded_videos, fill=year))+
geom_col(position = "dadge")
videos %>%
mutate(year = year(ymd_hms(contentDetails.videoPublishedAt)),
month = month(ymd_hms(contentDetails.videoPublishedAt)),
year = as.factor(year),
month = as.factor(month)
) %>%
group_by(year,month) %>%
summarise(uploaded_videos = n_distinct(contentDetails.videoId)) %>%
ggplot(aes(x=month, y=uploaded_videos, fill=year))+
geom_col(position = "dodge")
library(DT)
stats %>%
mutate(has_like = if_else(likeCount > 0, TRUE, FALSE)) %>%
filter(has_like) %>%
left_join(metadata, by = c("id"="video_id")) %>%
select(id, title, viewCount) %>%
datatable()
library(tm)
install.packages("tm")
install.packages("wordcloud")
library(tm)
library(wordcloud)
docs <- Corpus(VectorSource(metadata$title))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, "\\(")
docs <- tm_map(docs, toSpace, "\\)")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("spanish"))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("video",
"problema",
"ejemplo",
"parte",
"ejercicio",
"ejercicios",
"ejemplos"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=TRUE, rot.per=0.1,
colors=brewer.pal(8, "Dark2"))
library(readr)
library(dplyr)
library(leaflet)
library(crosstalk)
library(DT)
library(flexdashboard)
library(readr)
library(dplyr)
library(leaflet)
library(crosstalk)
library(DT)
quake<-read_csv("data/earthquakedate.csv")
quake<-read_csv("data/earthquakedate.csv")
quake<-read_csv("data/earthquakedate.csv")
quake<-read_csv("data/earthquakedata.csv")
quake<-read_csv("data/earthquakedate.csv")
quake<-read_csv("data/earthquakedata.csv")
quake %>%
filter(yr==1995) %>%
datatable()
quake %>%
filter(yr==1995) %>%
leaflet() %>%
addTiles() %>%
addMarkers(lng=~lon,
lat=~lat,
label=~mag)
shared_quake<-SharedData$new(quake %>% filter(yr>2000))
install.packages("htmltools")
install.packages("htmltools")
library(flexdashboard)
library(readr)
library(dplyr)
library(leaflet)
library(crosstalk)
library(DT)
quake<-read_csv("data/earthquakedata.csv")
quake %>%
filter(yr==1995) %>%
datatable()
quake %>%
filter(yr==1995) %>%
leaflet() %>%
addTiles() %>%
addMarkers(lng=~lon,
lat=~lat,
label=~mag)
shared_quake<-SharedData$new(quake %>% filter(yr>2000))
install_version("htmltools", "0.5.2")
install.versions(C('htmltools'), c('0.5.2'))
install.versions(C('htmltools'), c('0.5.2'))
install.versions(C('htmltools'), c('0.5.2'))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake <- SharedData$new(quake %>% filter(yr>2000))
packageVersion("htmltools")
library(htmltools)
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
remotes::install_github("rstudio/htmltools")
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
packageVersion("htmltools")
install.packages("htmltools")
install.packages("htmltools")
library(flexdashboard)
library(readr)
library(dplyr)
library(leaflet)
library(crosstalk)
library(DT)
shared_quake<- SharedData$new(quake %>% filter(yr>2000))
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
packerVersion("htmltools")
packageVersion("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
shiny::runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/ui_dinamico')
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/Tablas_DT')
runApp()
runApp()
runApp()
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/Tablas_DT')
?mtcars
mtcars
mtcars %>% DT::datatable(selection=list(mode='single', target='row'))
mtcars %>% DT::datatable(selection=list(mode='single', target='row'))
?datatable
?datatable
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/Tablas_DT')
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/plot_interacciones')
runApp()
runApp()
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/plot_interacciones')
runApp()
runApp('C:/Tasks_Log/Clase4-ShinyApps/shiny/plot_interacciones')

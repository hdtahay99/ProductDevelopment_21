#plot(hclust_avg)
#rect.hclust(hclust_avg , k = 3, border = 2:6)
#abline(h = 3, col = 'red')
#cut_avg=ifelse(cut_avg == 1,"coor1", cut_avg)
#cut_avg=ifelse(cut_avg == 2,"coor2", cut_avg)
#cut_avg=ifelse(cut_avg == 3,"coor3", cut_avg)
#cut_avg=ifelse(cut_avg == 4,"coor4", cut_avg)
#subData3<- data.frame(cut_avg) %>%
# dplyr::select(cut_avg)
#OHE3<-dummyVars("~.", data=subData3)
#OHE_dataframe3<-data.frame(predict(OHE3, newdata = subData3))
#OHE_dataframe3
set.seed(1699)
dist_coords2<-dist(test_scaled[, c(1,2)], method = 'euclidean')
hclust_avg2<-hclust(dist_coords2)
cut_avg<-cutree(hclust_avg2, k=4)
test_scaled=cbind(cut_avg, test_scaled)
#plot(hclust_avg)
#rect.hclust(hclust_avg , k = 3, border = 2:6)
#abline(h = 3, col = 'red')
#cut_avg=ifelse(cut_avg == 1,"coor1", cut_avg2)
#cut_avg=ifelse(cut_avg == 2,"coor2", cut_avg2)
#cut_avg=ifelse(cut_avg == 3,"coor3", cut_avg2)
#cut_avg=ifelse(cut_avg == 4,"coor4", cut_avg2)
#subData4<- data.frame(cut_avg) %>%
# dplyr::select(cut_avg)
#OHE4<-dummyVars("~.", data=subData4)
#OHE_dataframe4<-data.frame(predict(OHE4, newdata = subData4))
#OHE_dataframe4
dataset_train = cbind( OHE_dataframe, train_scaled, median_house_value=dataTrain$median_house_value)
dataset_test = cbind( OHE_dataframe2, test_scaled)
dataset_train$longitude=NULL
dataset_train$latitude=NULL
dataset_test$longitude=NULL
dataset_test$latitude=NULL
cor(dataset_train)
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
# Modelo back ward
step.model<-train(median_house_value ~., data = dataset_train,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:8),
trControl = train.control)
step.model$results
step.model$bestTune
summary(step.model$finalModel)
step.modelfinal<-lm(median_house_value ~ ocean_proximityINLAND  +ocean_proximityNEAR.BAY+cut_avg + housing_median_age+households +median_income + bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
summary(step.modelfinal)
predictBackWard<-dataTest[c("id")]
predictBackWard$median_house_value<-step.modelfinal %>% predict(dataset_test)
#write.csv(predictBackWard,"model_backward3.csv", row.names = FALSE)
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
# Modelo forward
step.model2<-train(median_house_value ~., data = dataset_train,
method = "leapSeq",
tuneGrid = data.frame(nvmax = 1:8),
trControl = train.control)
step.model2$results
step.model2$bestTune
summary(step.model2$finalModel)
step.model2$results
summary(step.model2$finalModel)
step.modelfinal2<-lm(median_house_value ~ ocean_proximityINLAND  + ocean_proximityNEAR.BAY + cut_avg   + housing_median_age  +households  +median_income  +bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
summary(step.modelfinal2)
predictForWard<-dataTest[c("id")]
predictForWard$median_house_value<-step.modelfinal2 %>% predict(dataset_test)
predictForWard
#write.csv(predictBackWard,"model_forward.csv", row.names = FALSE
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
step.model3<-train(median_house_value ~., data = dataset_train,
method = "lmStepAIC",
trControl = train.control,
trace = FALSE
)
step.model3$results
step.model3$finalModel
summary(step.model3$finalModel)
set.seed(222)
index<-createDataPartition(dataset_train$housing_median_age, p=0.8, list=FALSE)
dtrain<-dataset_train[index, ]
dtest<-dataset_train[-index, ]
dtrain
dtest
set.seed(123)
train_y<-dtrain[,'median_house_value']
train_x<-dtrain[, names(dtrain) !='median_house_value']
rf_model<-randomForest(train_x, y = train_y, ntree = 500, proximity=TRUE, importance = TRUE)
rf_model$importance
model_prediction = predict(rf_model, newdata = dtest[, names(dtrain) !='median_house_value'])
#rmse train local
rmse(dtest[,'median_house_value'], model_prediction)
#rmse train local
rmse(dtest[,'median_house_value'], model_prediction)
#test local
test_y = dtest[,'median_house_value']
test_x = dtest[, names(dtest) !='median_house_value']
y_pred = predict(rf_model , test_x)
rmse(test_y, y_pred)
dTestMatrix<-as.matrix(dataset_test)
frac_data<-dplyr::sample_frac(dataset_train, 0.2)
index<-as.numeric(rownames(frac_data))
train<-dataset_train[-index, ]
train
train_x<-as.matrix(dplyr::select(train, -median_house_value))
train_y<-train$median_house_value
gridBase<-expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
trainControlBase<-caret::trainControl(
method = "none",
verboseIter = FALSE,
allowParallel = TRUE
)
xgbBase<-caret::train(
x = train_x,
y = train_y,
trControl = trainControlBase,
tuneGrid = gridBase,
method = "xgbTree",
verbose = TRUE
)
preXGB<-predict(object = xgbBase, newdata = dTestMatrix)
summary(xgbBase)
#write.csv(preXGB,"model_xgb.csv", row.names = FALSE)
draw_tuning_plot<-function(x) {
ggplot(x) + coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = 0.90), min(x$results$RMSE))) +
theme_minimal()
}
nrounds<-1000
tuningGrid<-expand.grid(
nrounds          = seq(from = 200, to = nrounds, by = 50),
eta              = c(0.025, 0.05, 0.1, 0.3),
max_depth        = c(2, 3, 4, 5, 6),
gamma            = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample        = 1
)
tuningControl<-trainControl(
method        = "cv",
number        = 3,
verboseIter   = FALSE,
allowParallel = TRUE
)
xgbTune1<-train(
x = train_x,
y = train_y,
trControl = tuningControl,
tuneGrid = tuningGrid,
method = "xgbTree",
verbose = TRUE
)
draw_tuning_plot(xgbTune1)
xgbTune1$bestTune
tuningGrid2<- expand.grid(
nrounds          = seq(from = 50, to = nrounds, by = 50),
eta              = xgbTune1$bestTune$eta,
max_depth        = ifelse(xgbTune1$bestTune$max_depth == 2, c(xgbTune1$bestTune$max_depth:4),xgbTune1$bestTune$max_depth - 1:xgbTune1$bestTune$max_depth + 1),
gamma            = 0,
colsample_bytree = 1,
min_child_weight = c(1, 2, 3),
subsample        = 1
)
xgbTune2<-train(
x         = train_x,
y         = train_y,
trControl = tuningControl,
tuneGrid  = tuningGrid2,
method    = "xgbTree",
verbose   = TRUE
)
draw_tuning_plot(xgbTune2)
library(dplyr)
library(ggplot2)
library(caret)
library(MASS)
library(reshape2)
library(gridExtra)
library(Metrics)
library('randomForest')
library(corrplot)
dataTrain<-read.csv('train.csv')
dataTest<-read.csv('test.csv')
head(dataTrain)
head(dataTest)
summary(dataTrain)
summary(dataTest)
glimpse(dataTrain)
glimpse(dataTest)
draw_density_histogram<-function(column){
dataTrain %>%
ggplot(aes_string(x=column))+
geom_density(color="black", fill="#E69F00", alpha=0.4, lwd=0.7)+
xlab(column)+
ggtitle(paste("Density ", column, " plot"))+
theme_classic()
}
for (column in names(dataTrain[ , 2:10])){
print(draw_density_histogram(column))
}
draw_scatter_plot<-function(column){
dataTrain %>%
ggplot(aes_string(x=column, y="median_house_value"))+
geom_point(col="blue")+
ggtitle(paste("Scatter ", column, " plot"))+
theme_minimal()
}
for (column in names(dataTrain[ , 2:11])){
print(draw_scatter_plot(column))
}
colsNa<-colnames(dataTrain)[!complete.cases(t(dataTrain))]
colsNa
colsNa2<-colnames(dataTest)[!complete.cases(t(dataTest))]
colsNa2
incompleteData<-dataTrain %>% dplyr::select(colsNa)
incompleteData2<-dataTest %>% dplyr::select(colsNa2)
porcentajeNa<-as.data.frame(apply(incompleteData, MARGIN = 2, function(col) mean(is.na(col))))
colnames(porcentajeNa)<-"porcentaje"
porcentajeNa
porcentajeNa2<-as.data.frame(apply(incompleteData2, MARGIN = 2, function(col) mean(is.na(col))))
colnames(porcentajeNa2)<-"porcentaje"
porcentajeNa2
procesables<-porcentajeNa %>% filter(porcentajeNa < 0.05)
procesables2<-porcentajeNa2 %>% filter(porcentajeNa2 < 0.05)
incompleteData$total_bedrooms_media<-ifelse(is.na(incompleteData$total_bedrooms), mean(incompleteData$total_bedrooms, na.rm =TRUE),incompleteData$total_bedrooms)
incompleteData$total_bedrooms_median<-ifelse(is.na(incompleteData$total_bedrooms), median(incompleteData$total_bedrooms, na.rm =TRUE),incompleteData$total_bedrooms)
incompleteData
incompleteData2$total_bedrooms_media<-ifelse(is.na(incompleteData2$total_bedrooms), mean(incompleteData2$total_bedrooms, na.rm =TRUE),incompleteData2$total_bedrooms)
incompleteData2$total_bedrooms_median<-ifelse(is.na(incompleteData2$total_bedrooms), median(incompleteData2$total_bedrooms, na.rm =TRUE),incompleteData2$total_bedrooms)
incompleteData2
incompleteData %>%
ggplot(aes(x=total_bedrooms, y=..density..))+
geom_density(color="blue", lwd=1)+
geom_density(aes(x=total_bedrooms_media, y=..density..), col="red", lwd=1)+
geom_density(aes(x=total_bedrooms_median, y=..density..), col="purple", lwd=1)+
theme_minimal()
incompleteData2 %>%
ggplot(aes(x=total_bedrooms, y=..density..))+
geom_density(color="blue", lwd=1)+
geom_density(aes(x=total_bedrooms_media, y=..density..), col="red", lwd=1)+
geom_density(aes(x=total_bedrooms_median, y=..density..), col="purple", lwd=1)+
theme_minimal()
subd<-incompleteData %>%
dplyr::select(total_bedrooms, total_bedrooms_media, total_bedrooms_median)
dataX<-melt(subd)
dataX %>%
ggplot(aes(x=variable, y=value))+
geom_boxplot()
subd2<-incompleteData2 %>%
dplyr::select(total_bedrooms, total_bedrooms_media, total_bedrooms_median)
dataX2<-melt(subd2)
dataX2 %>%
ggplot(aes(x=variable, y=value))+
geom_boxplot()
dataTrain$total_bedrooms<-incompleteData$total_bedrooms_media
dataTest$total_bedrooms<-incompleteData2$total_bedrooms_media
dataCor<-cor(dataTrain[, -c(1,11)])
dataCor<-data.frame(dataCor)
names(dataCor)<-colnames(dataTrain[, -c(1,11)])
out<-corrplot(t(dataCor),method="pie",order="FPC")
dataTrain$bedrooms_mean<-dataTrain$total_bedrooms/dataTrain$households
dataTrain$rooms_mean<-dataTrain$total_rooms/dataTrain$households
dataTrain$population_mean<-dataTrain$population/dataTrain$households
dataTest$bedrooms_mean<-dataTest$total_bedrooms/dataTest$households
dataTest$rooms_mean<-dataTest$total_rooms/dataTest$households
dataTest$population_mean<-dataTest$population/dataTest$households
dataTrain$total_bedrooms<-NULL
dataTrain$total_rooms<-NULL
dataTrain$population<-NULL
dataTest$total_bedrooms<-NULL
dataTest$total_rooms<-NULL
dataTest$population<-NULL
head(dataTrain)
head(dataTest)
table(dataTrain$ocean_proximity)
table(dataTest$ocean_proximity)
subData<- dataTrain %>%
dplyr::select(ocean_proximity)
OHE<-dummyVars("~.", data=subData)
OHE_dataframe<-data.frame(predict(OHE, newdata = subData))
OHE_dataframe
subData2<- dataTest %>%
dplyr::select(ocean_proximity)
OHE2<-dummyVars("~.", data=subData2)
OHE_dataframe2<-data.frame(predict(OHE2, newdata = subData2))
OHE_dataframe2
train_scale<-dataTrain[, -c(1,7,8)]
test_scale<-dataTest[, -c(1,7)]
detect_outliers<-function(df, colname){
histPlot<-df %>%
ggplot(aes_string(x=colname))+
geom_histogram(color='white', fill='red', alpha=08)+
theme_minimal()
boxPlot<-df %>%
ggplot(aes_string(y=colname))+
geom_boxplot()+
theme_minimal()
qqPlot<- df %>%
ggplot(aes_string(sample=colname))+
stat_qq()+
stat_qq_line(col="red", lwd=1)+
theme_minimal()
plotOut<-grid.arrange(histPlot,boxPlot, qqPlot, ncol=3)+
theme(aspect.ratio = 5/50)
return(plotOut)
}
capping<-function(df,colname){
arr<-df[[colname]]
IQR<-quantile(arr, 0.7) - quantile(arr, 0.25)
# Superior a LS es un outlier, cuando no es normal
LS<-mean(arr) + 1.75*IQR
LI<-mean(arr) - 1.75*IQR
arr<-ifelse(arr >= LS, LS, arr)
arr<-ifelse(arr <= LI, LI, arr)
return(arr)
}
for (i in colnames(train_scale)){
detect_outliers(train_scale, i)
}
for (i in colnames(test_scale)){
detect_outliers(test_scale, i)
}
for(name in colnames(train_scale)){
train_scale[name]<-capping(train_scale, name)
}
for (i in colnames(train_scale)){
detect_outliers(train_scale, i)
}
for(name in colnames(test_scale)){
test_scale[name]<-capping(test_scale, name)
}
for (i in colnames(test_scale)){
detect_outliers(test_scale, i)
}
train_scaled<-scale(train_scale)
test_scaled<-scale(test_scale)
head(train_scaled)
head(test_scaled)
dataset_train = cbind( OHE_dataframe, train_scaled, median_house_value=dataTrain$median_house_value)
dataset_test = cbind( OHE_dataframe2, test_scaled)
cor(dataset_train)
# Primer modelo BaseLine
set.seed(2021)
index<-sample(1:nrow(dataset_train), nrow(dataset_test), replace = F)
localTrain<-dataset_train[-index, ]
localTest<-dataset_train[index, ]
predict1<-dataTest[c('id')]
predict1$median_house_value<-mean(localTrain$median_house_value)
#RMSE1<-sqrt(sum(((predict1$median_house_value - localTest$median_house_value)^2)/nrow(predict1)))
rmse(localTest$median_house_value, predict1$median_house_value)
#write.csv(predict1,"model_base1.csv", row.names = FALSE)
matchVal<-function(x){
index<-match(x, results$housing_median_age)
return(results[[index, "median_house_value"]])
}
results<-localTrain %>%
group_by(housing_median_age) %>%
summarise(median_house_value = mean(median_house_value, na.rm=TRUE))
predict2<-dataTest[c('id')]
predict2$median_house_value<-unlist(lapply(localTest$housing_median_age, matchVal))
RMSE2<-sqrt(sum(((predict2$median_house_value - localTest$median_house_value)^2)/nrow(predict2)))
rmse(localTest$median_house_value, predict2$median_house_value)
#write.csv(predict2,"model_base2.csv", row.names = FALSE)
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
# Modelo back ward
step.model<-train(median_house_value ~., data = dataset_train,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:8),
trControl = train.control)
step.model$results
step.model$bestTune
summary(step.model$finalModel)
step.modelfinal<-lm(median_house_value ~ ocean_proximityINLAND  +ocean_proximityNEAR.BAY+cut_avg + housing_median_age+households +median_income + bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
step.modelfinal<-lm(median_house_value ~ ocean_proximityINLAND+ longitude + latitude + housing_median_age+households +median_income + bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
summary(step.modelfinal)
predictBackWard<-dataTest[c("id")]
predictBackWard$median_house_value<-step.modelfinal %>% predict(dataset_test)
#write.csv(predictBackWard,"model_backward3.csv", row.names = FALSE)
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
# Modelo forward
step.model2<-train(median_house_value ~., data = dataset_train,
method = "leapSeq",
tuneGrid = data.frame(nvmax = 1:8),
trControl = train.control)
step.model2$results
step.model2$bestTune
summary(step.model2$finalModel)
summary(step.model2$finalModel)
step.modelfinal2<-lm(median_house_value ~ ocean_proximityINLAND  + longitude + latitude + housing_median_age  +households  +median_income  +bedrooms_mean  + rooms_mean + population_mean, data = dataset_train)
summary(step.modelfinal2)
predictForWard<-dataTest[c("id")]
predictForWard$median_house_value<-step.modelfinal2 %>% predict(dataset_test)
predictForWard
#write.csv(predictBackWard,"model_forward.csv", row.names = FALSE
set.seed(2021)
# K-fold cross validation
train.control<-trainControl(method = "cv", number = 10)
step.model3<-train(median_house_value ~., data = dataset_train,
method = "lmStepAIC",
trControl = train.control,
trace = FALSE
)
step.model3$results
step.model3$finalModel
summary(step.model3$finalModel)
![LINK VIDEO](https://youtu.be/Og6Gy_MVTiM)
# Descripción del Problema
El dataset contiene información sobre el censo de vivienda del estado de California de 1990, cada fila o instancia de la misma representa la información recabada por bloque; es decir un conjunto de casas, los campos o variables que la conforman son las siguientes:
library(forecast)
library(tseries)
library(dplyr)
library(xts)
library(lubridate)
set.seed(2021)
#1. Cree un arreglo de 1500 valores con distribución de probabilidad normal con media = 0 y desviación estándar = 5.
arrNorm=rnorm(n=1500, mean=0, sd=5)
# 2. Cree una suma acumulativa de estos valores.
cumNorm = cumsum(arrNorm)
# 3. Muestre una gráfica de la serie temporal.
plot(cumNorm)
# 4. Cree una serie de tiempo con la función ts()
miTS = ts(cumNorm, frequency = 12)
# 5. Verifique la descomposición de la serie tiempo usando una frecuencia de 12.
plot(decompose(miTS))
# 6. Realice un comentario sobre cada uno de los componentes de la serie temporal.
# Tendencia - la gráfica de la tendencia al principio o inclusive antes de llegar a la "mitad" de los datos que se tienen sobre el tiempo, no está tan constante, baja, trata de mantenerse luego tiene una subida hasta que llega al punto que después de bajar logra compensar ello y empieza a ver un incremento hasta cierto punto y trata de estabilizarse hasta final, probablemente, de acuerdo al final de la tendencia, algunas de las aproximaciones a predecir en el forecast muestren una subida o al menos "en subida".
# Seasonality - Si se expande la gráfica de seasonality, se puede ver claro que a cada cierto tiempo hay una estacionalidad, lo cual demuestra que si existe ese patrón en el caso de tal serie temporal.
# Random- Se puede comprobar también que la serie temporal tiene el componente estocástico, que comúnmente se conoce como el componente que no se puede explicar, y se logra apreciar en aquellos puntos de la observación de que no son líneas suaves, sino que claramente serían lo que se logra observar en random.
# 7. Determine se la serie es estacionar o no (stationarity).
adf.test(miTS)
## El valor p, es bajo, pero ni siquiera tan bajo, para considerar que la serie es stationary, inclusive con evaluarla, no se logra apreciar que exista stationary porque no se mantiene constante, por lo que se concluye que no es una serie STATIONARY.
# 8. De no ser estacionar, investigue como es posible resolver este problema usando diferenciación temporal.
diffMiTS = diff(miTS)
plot(decompose(diffMiTS))
adf.test(diffMiTS)
# 9. Muestre y una grafica sobre la serie temporal resultante.
plot(diffMiTS)
df = read.csv("dataOut.csv")
df
unique(df$id_producto)
table(df$id_producto)
## Debido a que son 84 observaciones por cada id producto, en los pasos indicados en el pdf, dicen "38" observaciones, por lo que se tomará las últimas 38 observaciones, sabiendo que cada observación corresponde a un mes en específico.
reproducir_arima = function(df, id){
temp_df = df %>%
filter(id_producto == id) %>%
top_n(38, timestamp) %>%
select(timestamp, produccion)
temp_df$timestamp = ymd(temp_df$timestamp)
temp_ts = ts(temp_df$produccion,start = c(2018, 11), frequency = 12)
temp_ts = tsclean(temp_ts)
diffMiTS = diff(temp_ts)
bestModel = auto.arima(diffMiTS, trace = F)
plot(forecast(bestModel, h=16))
}
for(id in unique(df$id_producto)){
print(id)
reproducir_arima(df, id)
}
install.packages("Rcpp")
plot(cars)
head(mtcars)
x <- sample(letters,replace=TRUE,size=10)
print(x)
plot(cars)
reticulate::repl_python()
library(reticulate)
Y
head(mtcars)
x <- sample(letters,replace=TRUE,size=10)
print(x)
num<- 1+3
plot(cars)
library(reticulate)
reticulate::repl_python()
x = 42 * 2
print(x)
quit
#conda_install("r-reticulate", "pandas")
use_condaenv("r-reticulate")
reticulate::repl_python()
import pandas
iris_pandas = pandas.read_csv("iris.txt")
iris_setosa = iris_pandas[iris_pandas['species'] == "setosa"]
iris_setosa.head()
quit
library(ggplot2)
library(dplyr)
py$iris_setosa %>%
ggplot(aes(x=sepal_length,y=sepal_width))+
geom_point()
library(RMySQL)
dbcon <- dbConnect(MySQL(),
host = '127.0.0.1',
user = 'root',
password = 'D5ekyngy',
dbname = 'agroserviciodb')
dbDisconnect(dbcon)
